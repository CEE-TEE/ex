{{- define "deploy" -}}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-{{ .name }}
  labels:
    chart: {{ .Chart.Name }}-{{ .Chart.Version | replace "+" "-" }}
spec:
  {{- if not .Values.canary }}
  {{- if not (hasKey . "autoscaling") }}
  replicas: {{ .replicas }}
  {{- end }}
  {{- else }}
  replicas: 1
  {{- end }}
  strategy:
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 50%
  selector:
    matchLabels:
      app: "{{ .Chart.Name }}-{{ .name }}"
      release: "{{ .Release.Name }}"
  template:
    metadata:
      labels:
        app: "{{ .Chart.Name }}-{{ .name }}"
        name: {{ .Chart.Name }}-{{ .Chart.Version | replace "+" "-" }}
        release: "{{ .Release.Name }}"
        app.kubernetes.io/component: service
    spec:
      volumes:
        - name: secrets
          secret:
            secretName: your-service-secrets-v1
      containers:
      - name: "{{ .name }}"
        image: "{{ .Values.deployment.image }}"
        imagePullPolicy: IfNotPresent
        command: ["{{ .cmd }}"]
        volumeMounts:
          - name: secrets
            readOnly: true
            mountPath: /secrets

        env:
          # Common environment variables
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName

          - name: O11Y_HONEYCOMB_KEY
            valueFrom:
              secretKeyRef:
                name: your-service-secrets-v1
                key: honeycomb.writekey
          - name: O11Y_ROLLBAR_TOKEN
            valueFrom:
              secretKeyRef:
                name: your-service-secrets-v1
                key: rollbar.token

        {{- if hasKey . "env" }}
{{ toYaml .env | indent 10 }}
        {{- end }}

        ports:
          # Common ports
          - containerPort: 8001
            name: admin
            protocol: TCP
          {{- if hasKey . "ports" }}
          # Deployment-specific ports
{{ toYaml .ports | indent 10 }}
          {{- end }}

        resources:
          limits:
            {{- if hasKey . "cpuQuota" }}
              # Deployment-specific cpu
            cpu: {{ .cpuQuota }}
            {{- else }}
            cpu: "0.5"
            {{- end }}

            memory: 512M
          requests:
            {{- if hasKey . "cpuQuota" }}
            # Deployment-specific cpu
            cpu: {{ .cpuQuota }}
            {{- else }}
            cpu: "0.5"
            {{- end }}
            memory: 512M

        livenessProbe:
          httpGet:
            path: /live
            scheme: HTTP
            port: admin
          initialDelaySeconds: 10
          timeoutSeconds: 10
          successThreshold: 1
          failureThreshold: 6

        readinessProbe:
          httpGet:
            path: /ready
            scheme: HTTP
            port: admin
          initialDelaySeconds: 10
          timeoutSeconds: 10
          successThreshold: 1
          failureThreshold: 2

{{- if not .Values.canary }}
{{- if hasKey . "autoscaling" }}
---
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ .Release.Name }}-{{ .name }}
  labels:
    chart: {{ .Chart.Name }}-{{ .Chart.Version | replace "+" "-" }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ .Release.Name }}-{{ .name }}
  minReplicas: {{ .autoscaling.minReplicas | default 2 }}
  maxReplicas: {{ .autoscaling.maxReplicas | default 8 }}
  metrics:
  {{- if hasKey .autoscaling "metrics" }}
{{ toYaml .autoscaling.metrics | indent 4}}
  {{- else}}
    - type: Resource
      resource:
        # Target 70% of utilized memory
        name: memory
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        # Target 70% of utilized CPU
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
  {{- end }}
  behavior:
{{- if hasKey .autoscaling "behavior" }}
{{ toYaml .autoscaling.behavior | indent 4}}
  {{- else}}
    scaleDown:
      # When the metrics indicate that the target should be scaled down the algorithm looks into previously
      # computed desired states and uses the highest value from the specified interval - 5 minutes
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          # Scale down a max of 50% of the running pods
          value: 50
          # per 2 minutes
          periodSeconds: 120
    scaleUp:
      # do we need to scale up? do the math based on the previous 2 min.
      stabilizationWindowSeconds: 120
      policies:
        - type: Percent
          value: 50
          # if new need to add a pod, do it every 15 secs
          periodSeconds: 15
        - type: Pods
          # maximum of 4 pods added at once
          value: 4
          # per 15 seconds
          periodSeconds: 15
      selectPolicy: Max # the largest of all defined policies is selected for scale up
  {{- end }}
{{- end }}
{{- end }}

{{- end -}}